import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import random
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

# =====================================================
# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# =====================================================

@st.cache_data
def load_public_data():
    """ê³µê³µë°ì´í„° ë¡œë“œ ë° ì¸ì½”ë”© ì²˜ë¦¬"""
    file_path = r"G:\ë‚´ ë“œë¼ì´ë¸Œ\python\Sesac\ìµœì¢… í”„ë¡œì íŠ¸\ì„œìš¸ì‹œ ìƒê¶Œë¶„ì„ì„œë¹„ìŠ¤(ì¶”ì •ë§¤ì¶œ-í–‰ì •ë™).csv"
    
    # ì—¬ëŸ¬ ì¸ì½”ë”© ì‹œë„
    encodings = ['cp949', 'euc-kr', 'utf-8', 'cp1252']
    
    for encoding in encodings:
        try:
            df = pd.read_csv(file_path, encoding=encoding)
            print(f"âœ… íŒŒì¼ ë¡œë“œ ì„±ê³µ (ì¸ì½”ë”©: {encoding})")
            
            # ì»¬ëŸ¼ëª… ì •ë¦¬ (ê¹¨ì§„ í•œê¸€ ì²˜ë¦¬)
            df.columns = [
                'year_quarter', 'dong_code', 'dong_name', 'service_code', 'service_name',
                'monthly_sales', 'monthly_count', 'weekday_sales', 'weekend_sales',
                'mon_sales', 'tue_sales', 'wed_sales', 'thu_sales', 'fri_sales', 'sat_sales', 'sun_sales',
                'time_00_06_sales', 'time_06_11_sales', 'time_11_14_sales', 'time_14_17_sales', 
                'time_17_21_sales', 'time_21_24_sales',
                'male_sales', 'female_sales',
                'age_10_sales', 'age_20_sales', 'age_30_sales', 'age_40_sales', 'age_50_sales', 'age_60_sales',
                'weekday_count', 'weekend_count',
                'mon_count', 'tue_count', 'wed_count', 'thu_count', 'fri_count', 'sat_count', 'sun_count',
                'time_00_06_count', 'time_06_11_count', 'time_11_14_count', 'time_14_17_count',
                'time_17_21_count', 'time_21_24_count',
                'male_count', 'female_count',
                'age_10_count', 'age_20_count', 'age_30_count', 'age_40_count', 'age_50_count', 'age_60_count'
            ]
            
            return df
        except:
            continue
    
    st.error("íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    return None

@st.cache_data
def create_virtual_activity_data():
    """ê°€ìƒ í™œë™ ë°ì´í„° ìƒì„± (1ë…„ì¹˜, 10000í–‰)"""
    print("ğŸ”„ ê°€ìƒ í™œë™ ë°ì´í„° ìƒì„± ì¤‘...")
    
    # ì‹¤ì œ ë™ ë¦¬ìŠ¤íŠ¸ (ì„œìš¸ ì£¼ìš” ìƒê¶Œ)
    dong_list = ['ì—­ì‚¼1ë™', 'ë…¼í˜„1ë™', 'ì²­ë‹´ë™', 'ì‚¼ì„±1ë™', 'ëŒ€ì¹˜2ë™',  # ê°•ë‚¨
                 'ì„œêµë™', 'ì—°ë‚¨ë™', 'ìƒìˆ˜ë™', 'í•©ì •ë™', 'ë§ì›1ë™',      # ë§ˆí¬
                 'ì„±ìˆ˜1ê°€1ë™', 'ì„±ìˆ˜1ê°€2ë™', 'ì„±ìˆ˜2ê°€1ë™',              # ì„±ë™
                 'ëª…ë™', 'ì„ì§€ë¡œë™', 'íšŒí˜„ë™', 'í•„ë™',                   # ì¤‘êµ¬
                 'ì´íƒœì›1ë™', 'í•œë‚¨ë™', 'ìš©ì‚°2ê°€ë™']                    # ìš©ì‚°
    
    data = []
    store_id_counter = 1
    
    # ë™ë³„ íŠ¹ì„± ì„¤ì • (íšŒì „ìœ¨ ì°¨ì´ ë°˜ì˜)
    dong_characteristics = {
        'ì„±ìˆ˜1ê°€1ë™': {'ì°½ì—…ë¥ ': 0.15, 'íì—…ë¥ ': 0.08},  # í•«í”Œë ˆì´ìŠ¤
        'ì„±ìˆ˜2ê°€1ë™': {'ì°½ì—…ë¥ ': 0.14, 'íì—…ë¥ ': 0.07},
        'ì—°ë‚¨ë™': {'ì°½ì—…ë¥ ': 0.13, 'íì—…ë¥ ': 0.09},
        'ì„œêµë™': {'ì°½ì—…ë¥ ': 0.12, 'íì—…ë¥ ': 0.10},
        'ì´íƒœì›1ë™': {'ì°½ì—…ë¥ ': 0.11, 'íì—…ë¥ ': 0.12},  # ì½”ë¡œë‚˜ ì˜í–¥
        'ì—­ì‚¼1ë™': {'ì°½ì—…ë¥ ': 0.08, 'íì—…ë¥ ': 0.05},    # ì•ˆì •ì 
        'ëŒ€ì¹˜2ë™': {'ì°½ì—…ë¥ ': 0.06, 'íì—…ë¥ ': 0.04},
        'ëª…ë™': {'ì°½ì—…ë¥ ': 0.10, 'íì—…ë¥ ': 0.15},       # ê´€ê´‘ì§€ ë³€ë™ì„±
    }
    
    # 1ë…„ê°„ ì¼ë³„ ë°ì´í„° ìƒì„±
    start_date = datetime.now() - timedelta(days=365)
    
    for dong in dong_list:
        # ë™ë³„ ê¸°ë³¸ ìƒì  ìˆ˜ ì„¤ì •
        base_stores = random.randint(50, 200)
        active_stores = set(range(store_id_counter, store_id_counter + base_stores))
        store_id_counter += base_stores
        
        # ë™ë³„ íŠ¹ì„± ê°€ì ¸ì˜¤ê¸°
        char = dong_characteristics.get(dong, {'ì°½ì—…ë¥ ': 0.08, 'íì—…ë¥ ': 0.06})
        
        for day in range(365):
            current_date = start_date + timedelta(days=day)
            
            # ì›”ë³„ë¡œ ì°½ì—…/íì—… ì´ë²¤íŠ¸ ë°œìƒ
            if day % 30 == 0:
                # ì°½ì—…
                new_stores = int(len(active_stores) * char['ì°½ì—…ë¥ '])
                for _ in range(new_stores):
                    active_stores.add(store_id_counter)
                    store_id_counter += 1
                
                # íì—…
                close_stores = int(len(active_stores) * char['íì—…ë¥ '])
                if close_stores > 0 and len(active_stores) > 10:
                    stores_to_close = random.sample(list(active_stores), min(close_stores, len(active_stores)-10))
                    for store in stores_to_close:
                        active_stores.discard(store)
            
            # ì¼ë³„ ê±°ë˜ ìƒì„±
            daily_transactions = random.randint(len(active_stores)*2, len(active_stores)*5)
            
            for _ in range(daily_transactions // len(dong_list)):  # ë°ì´í„°ëŸ‰ ì¡°ì ˆ
                if active_stores:
                    data.append({
                        'transaction_date': current_date,
                        'store_id': random.choice(list(active_stores)),
                        'dong_name': dong,
                        'transaction_amount': random.randint(5000, 100000),
                        'is_new': random.random() < char['ì°½ì—…ë¥ '],  # ì‹ ê·œ ìƒì  í”Œë˜ê·¸
                        'is_closing': random.random() < char['íì—…ë¥ ']  # íì—… ì˜ˆì • í”Œë˜ê·¸
                    })
    
    df = pd.DataFrame(data)
    print(f"âœ… ê°€ìƒ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(df):,}í–‰")
    return df

@st.cache_data
def create_analysis_tables(public_df, virtual_df):
    """ì „ì²˜ë¦¬ëœ ë¶„ì„ìš© í…Œì´ë¸” ìƒì„±"""
    
    # 1. ìƒê¶Œ íŠ¹ì„± í…Œì´ë¸” (ê³µê³µë°ì´í„° ê¸°ë°˜)
    area_features = public_df.groupby('dong_name').agg({
        'service_name': lambda x: len(x.unique()),  # ì—…ì¢… ë‹¤ì–‘ì„±
        'monthly_sales': 'sum',
        'monthly_count': 'sum',
        'weekday_sales': 'sum',
        'weekend_sales': 'sum',
        'age_20_sales': 'sum',
        'age_30_sales': 'sum',
        'age_40_sales': 'sum',
        'male_sales': 'sum',
        'female_sales': 'sum'
    }).reset_index()
    
    # íŒŒìƒ ë³€ìˆ˜ ìƒì„±
    area_features['ì—…ì¢…_ë‹¤ì–‘ì„±_ì§€ìˆ˜'] = area_features['service_name'] / area_features['service_name'].max()
    area_features['ì£¼ì¤‘_ë§¤ì¶œ_ë¹„ìœ¨'] = area_features['weekday_sales'] / (area_features['weekday_sales'] + area_features['weekend_sales'])
    area_features['í‰ê· _ê²°ì œ_ê¸ˆì•¡'] = area_features['monthly_sales'] / area_features['monthly_count']
    
    # ì£¼ìš” ì—°ë ¹ëŒ€ ê³„ì‚°
    age_cols = ['age_20_sales', 'age_30_sales', 'age_40_sales']
    area_features['ì£¼ìš”_ì—°ë ¹ëŒ€'] = area_features[age_cols].idxmax(axis=1).str.extract('(\d+)')[0].astype(int)
    area_features['ì—¬ì„±_ë§¤ì¶œ_ë¹„ìœ¨'] = area_features['female_sales'] / (area_features['male_sales'] + area_features['female_sales'])
    
    # 2. ìƒê¶Œ í™œë™ì„± í…Œì´ë¸” (ê°€ìƒ ë°ì´í„° ê¸°ë°˜)
    # ì›”ë³„ ì°½ì—…/íì—… ê³„ì‚°
    virtual_df['year_month'] = virtual_df['transaction_date'].dt.to_period('M')
    
    activity_features = virtual_df.groupby('dong_name').agg({
        'store_id': lambda x: len(x.unique()),
        'is_new': 'sum',
        'is_closing': 'sum',
        'transaction_amount': 'mean'
    }).reset_index()
    
    activity_features.columns = ['dong_name', 'total_stores', 'new_stores', 'closed_stores', 'avg_transaction']
    activity_features['ì‹ ê·œ_ì°½ì—…ë¥ '] = activity_features['new_stores'] / activity_features['total_stores']
    activity_features['íì—…ë¥ '] = activity_features['closed_stores'] / activity_features['total_stores']
    activity_features['ìƒê¶Œ_íšŒì „ìœ¨'] = activity_features['ì‹ ê·œ_ì°½ì—…ë¥ '] + activity_features['íì—…ë¥ ']
    
    # ë‘ í…Œì´ë¸” ë³‘í•©
    final_df = area_features.merge(activity_features, on='dong_name', how='inner')
    
    return area_features, activity_features, final_df

# =====================================================
# 2. ë¶„ì„ í•¨ìˆ˜ë“¤
# =====================================================

def perform_clustering(df):
    """1ë‹¨ê³„: ìƒê¶Œ ìœ í˜• ë¶„ë¥˜"""
    
    # í´ëŸ¬ìŠ¤í„°ë§ìš© íŠ¹ì„± ì„ íƒ
    features = ['ì—…ì¢…_ë‹¤ì–‘ì„±_ì§€ìˆ˜', 'ì£¼ì¤‘_ë§¤ì¶œ_ë¹„ìœ¨', 'í‰ê· _ê²°ì œ_ê¸ˆì•¡', 'ì£¼ìš”_ì—°ë ¹ëŒ€', 'ì—¬ì„±_ë§¤ì¶œ_ë¹„ìœ¨']
    X = df[features].fillna(0)
    
    # ì •ê·œí™”
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # K-means í´ëŸ¬ìŠ¤í„°ë§
    kmeans = KMeans(n_clusters=4, random_state=42)
    df['cluster'] = kmeans.fit_predict(X_scaled)
    
    # í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ë¶„ì„
    cluster_profiles = df.groupby('cluster')[features].mean()
    
    # í´ëŸ¬ìŠ¤í„° ì´ë¦„ ë¶€ì—¬
    cluster_names = {
        0: 'íŠ¸ë Œë””í˜• ìƒê¶Œ',
        1: 'ì£¼ê±°ë°€ì§‘í˜• ìƒê¶Œ',
        2: 'ì˜¤í”¼ìŠ¤í˜• ìƒê¶Œ',
        3: 'ì „í†µì‹œì¥í˜• ìƒê¶Œ'
    }
    
    # ê°€ì¥ íŠ¸ë Œë””í•œ í´ëŸ¬ìŠ¤í„° ì°¾ê¸° (20ëŒ€ ë¹„ì¤‘ + ì—…ì¢… ë‹¤ì–‘ì„±ì´ ë†’ì€)
    trendy_cluster = cluster_profiles.nlargest(1, ['ì—…ì¢…_ë‹¤ì–‘ì„±_ì§€ìˆ˜']).index[0]
    cluster_names[trendy_cluster] = 'ğŸ”¥ íŠ¸ë Œë””í˜• ìƒê¶Œ'
    
    df['cluster_name'] = df['cluster'].map(cluster_names)
    
    return df, cluster_profiles, cluster_names

def build_prediction_model(df):
    """2ë‹¨ê³„: ìœ ë§ ìƒê¶Œ ì˜ˆì¸¡ ëª¨ë¸"""
    
    # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
    feature_cols = ['ì—…ì¢…_ë‹¤ì–‘ì„±_ì§€ìˆ˜', 'ì£¼ì¤‘_ë§¤ì¶œ_ë¹„ìœ¨', 'í‰ê· _ê²°ì œ_ê¸ˆì•¡', 
                   'ì£¼ìš”_ì—°ë ¹ëŒ€', 'ì—¬ì„±_ë§¤ì¶œ_ë¹„ìœ¨', 'monthly_sales']
    
    X = df[feature_cols].fillna(0)
    y = df['ìƒê¶Œ_íšŒì „ìœ¨'].fillna(df['ìƒê¶Œ_íšŒì „ìœ¨'].mean())
    
    # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # Random Forest ëª¨ë¸
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    # ì˜ˆì¸¡
    df['predicted_turnover'] = model.predict(X)
    
    # íŠ¹ì„± ì¤‘ìš”ë„
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    return df, model, feature_importance

def find_top_areas(df):
    """3ë‹¨ê³„: ìµœìš°ì„  ë³´ê¸‰ ì§€ì—­ ì„ ì •"""
    
    # íŠ¸ë Œë””í˜• ìƒê¶Œ ì¤‘ íšŒì „ìœ¨ ë†’ì€ ê³³
    trendy_areas = df[df['cluster_name'].str.contains('íŠ¸ë Œë””')].copy()
    
    # ì¢…í•© ì ìˆ˜ ê³„ì‚°
    trendy_areas['opportunity_score'] = (
        trendy_areas['predicted_turnover'] * 0.4 +  # ì˜ˆì¸¡ íšŒì „ìœ¨
        trendy_areas['ì‹ ê·œ_ì°½ì—…ë¥ '] * 0.3 +          # ì‹¤ì œ ì°½ì—…ë¥ 
        trendy_areas['ì—…ì¢…_ë‹¤ì–‘ì„±_ì§€ìˆ˜'] * 0.2 +     # ë‹¤ì–‘ì„±
        (1 - trendy_areas['í‰ê· _ê²°ì œ_ê¸ˆì•¡'] / trendy_areas['í‰ê· _ê²°ì œ_ê¸ˆì•¡'].max()) * 0.1  # ë‚®ì€ ê°ë‹¨ê°€ ì„ í˜¸
    )
    
    # ìƒìœ„ 5ê°œ ì§€ì—­
    top_areas = trendy_areas.nlargest(5, 'opportunity_score')[
        ['dong_name', 'cluster_name', 'predicted_turnover', 'ì‹ ê·œ_ì°½ì—…ë¥ ', 'opportunity_score']
    ]
    
    return top_areas, trendy_areas

# =====================================================
# 3. Streamlit ì•±
# =====================================================

def main():
    st.set_page_config(page_title="í† ìŠ¤í”Œë ˆì´ìŠ¤ ìƒê¶Œ ë¶„ì„", layout="wide")
    
    st.title("ğŸš€ í† ìŠ¤í”Œë ˆì´ìŠ¤ ìƒê¶Œ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
    st.markdown("### ë°ì´í„° ê¸°ë°˜ ì‹ ê·œ ë‹¨ë§ê¸° ë³´ê¸‰ ì „ëµ")
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.image("https://static.toss.im/assets/toss-place/tossplace-logo.png", width=200)
        st.markdown("---")
        st.markdown("### ğŸ“Š ë¶„ì„ í”„ë¡œì„¸ìŠ¤")
        st.info("""
        1ï¸âƒ£ ìƒê¶Œ ìœ í˜• ë¶„ë¥˜
        2ï¸âƒ£ ìœ ë§ ìƒê¶Œ ì˜ˆì¸¡
        3ï¸âƒ£ ìµœìš°ì„  ì§€ì—­ ë„ì¶œ
        """)
    
    # ë°ì´í„° ë¡œë“œ
    with st.spinner("ë°ì´í„° ë¡œë”© ì¤‘..."):
        public_df = load_public_data()
        
        if public_df is None:
            st.error("ê³µê³µë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            st.stop()
        
        virtual_df = create_virtual_activity_data()
        area_features, activity_features, final_df = create_analysis_tables(public_df, virtual_df)
    
    # ë¶„ì„ ìˆ˜í–‰
    with st.spinner("ë¶„ì„ ì¤‘..."):
        # 1ë‹¨ê³„: í´ëŸ¬ìŠ¤í„°ë§
        final_df, cluster_profiles, cluster_names = perform_clustering(final_df)
        
        # 2ë‹¨ê³„: ì˜ˆì¸¡ ëª¨ë¸
        final_df, model, feature_importance = build_prediction_model(final_df)
        
        # 3ë‹¨ê³„: ìµœìš°ì„  ì§€ì—­
        top_areas, trendy_areas = find_top_areas(final_df)
    
    # íƒ­ êµ¬ì„±
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š 1ë‹¨ê³„: ìƒê¶Œ ë¶„ë¥˜", "ğŸ”® 2ë‹¨ê³„: ìœ ë§ ì˜ˆì¸¡", "ğŸ¯ 3ë‹¨ê³„: ìµœìš°ì„  ì§€ì—­", "ğŸ’¡ ìµœì¢… ì œì•ˆ"])
    
    with tab1:
        st.header("1ë‹¨ê³„: ìƒê¶Œ ìœ í˜• ë¶„ë¥˜")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # í´ëŸ¬ìŠ¤í„° ë¶„í¬
            fig = px.scatter(final_df, 
                           x='ì—…ì¢…_ë‹¤ì–‘ì„±_ì§€ìˆ˜', 
                           y='í‰ê· _ê²°ì œ_ê¸ˆì•¡',
                           color='cluster_name',
                           size='monthly_sales',
                           hover_data=['dong_name'],
                           title="ìƒê¶Œ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼")
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # í´ëŸ¬ìŠ¤í„°ë³„ íŠ¹ì„±
            fig = go.Figure()
            for col in ['ì—…ì¢…_ë‹¤ì–‘ì„±_ì§€ìˆ˜', 'ì£¼ì¤‘_ë§¤ì¶œ_ë¹„ìœ¨', 'ì—¬ì„±_ë§¤ì¶œ_ë¹„ìœ¨']:
                fig.add_trace(go.Bar(
                    name=col,
                    x=list(cluster_names.values()),
                    y=cluster_profiles[col].values
                ))
            fig.update_layout(title="í´ëŸ¬ìŠ¤í„°ë³„ íŠ¹ì„± ë¹„êµ", barmode='group')
            st.plotly_chart(fig, use_container_width=True)
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ë™ ë¦¬ìŠ¤íŠ¸
        st.subheader("ğŸª í´ëŸ¬ìŠ¤í„°ë³„ ìƒê¶Œ ë¶„í¬")
        for cluster_name in cluster_names.values():
            areas = final_df[final_df['cluster_name'] == cluster_name]['dong_name'].head(5).tolist()
            if areas:
                st.write(f"**{cluster_name}**: {', '.join(areas)}")
    
    with tab2:
        st.header("2ë‹¨ê³„: ìœ ë§ ìƒê¶Œ ì˜ˆì¸¡ ëª¨ë¸")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # íŠ¹ì„± ì¤‘ìš”ë„
            fig = px.bar(feature_importance, 
                        x='importance', 
                        y='feature',
                        orientation='h',
                        title="ìƒê¶Œ íšŒì „ìœ¨ ì˜ˆì¸¡ - íŠ¹ì„± ì¤‘ìš”ë„")
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # ì‹¤ì œ vs ì˜ˆì¸¡ íšŒì „ìœ¨
            fig = px.scatter(final_df,
                           x='ìƒê¶Œ_íšŒì „ìœ¨',
                           y='predicted_turnover',
                           color='cluster_name',
                           hover_data=['dong_name'],
                           title="ì‹¤ì œ vs ì˜ˆì¸¡ íšŒì „ìœ¨")
            fig.add_trace(go.Scatter(x=[0, 0.3], y=[0, 0.3], 
                                    mode='lines', 
                                    name='Perfect Prediction',
                                    line=dict(dash='dash')))
            st.plotly_chart(fig, use_container_width=True)
        
        # ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½
        st.subheader("ğŸ“ˆ ì˜ˆì¸¡ ëª¨ë¸ ì¸ì‚¬ì´íŠ¸")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("í‰ê·  íšŒì „ìœ¨", f"{final_df['ìƒê¶Œ_íšŒì „ìœ¨'].mean():.2%}")
        with col2:
            st.metric("ìµœê³  íšŒì „ìœ¨ ì§€ì—­", 
                     final_df.nlargest(1, 'predicted_turnover')['dong_name'].values[0])
        with col3:
            st.metric("ê°€ì¥ ì¤‘ìš”í•œ ìš”ì¸", 
                     feature_importance.iloc[0]['feature'])
    
    with tab3:
        st.header("3ë‹¨ê³„: ìµœìš°ì„  ë³´ê¸‰ ì§€ì—­")
        
        # ìƒìœ„ 5ê°œ ì§€ì—­ ì¹´ë“œ
        st.subheader("ğŸ† TOP 5 ìœ ë§ ìƒê¶Œ")
        
        cols = st.columns(5)
        for idx, (_, row) in enumerate(top_areas.iterrows()):
            with cols[idx]:
                st.metric(
                    label=f"#{idx+1}",
                    value=row['dong_name'],
                    delta=f"íšŒì „ìœ¨ {row['predicted_turnover']:.1%}"
                )
        
        # ì¢…í•© ì ìˆ˜ ì‹œê°í™”
        fig = make_subplots(
            rows=1, cols=2,
            subplot_titles=("ê¸°íšŒ ì ìˆ˜ TOP 10", "ì°½ì—…ë¥  vs ì˜ˆì¸¡ íšŒì „ìœ¨")
        )
        
        # ë§‰ëŒ€ ì°¨íŠ¸
        top10 = final_df.nlargest(10, 'opportunity_score' if 'opportunity_score' in final_df.columns else 'predicted_turnover')
        fig.add_trace(
            go.Bar(x=top10['dong_name'], 
                  y=top10['predicted_turnover'],
                  name='ì˜ˆì¸¡ íšŒì „ìœ¨'),
            row=1, col=1
        )
        
        # ì‚°ì ë„
        fig.add_trace(
            go.Scatter(x=final_df['ì‹ ê·œ_ì°½ì—…ë¥ '],
                      y=final_df['predicted_turnover'],
                      mode='markers+text',
                      text=final_df['dong_name'],
                      textposition="top center",
                      marker=dict(size=10, color=final_df['monthly_sales'], colorscale='Viridis'),
                      name='ìƒê¶Œë³„ ìœ„ì¹˜'),
            row=1, col=2
        )
        
        fig.update_layout(height=400, showlegend=False)
        st.plotly_chart(fig, use_container_width=True)
        
        # ìƒì„¸ í…Œì´ë¸”
        st.subheader("ğŸ“‹ ìƒìœ„ ì§€ì—­ ìƒì„¸ ì •ë³´")
        display_cols = ['dong_name', 'cluster_name', 'ì‹ ê·œ_ì°½ì—…ë¥ ', 'íì—…ë¥ ', 
                       'í‰ê· _ê²°ì œ_ê¸ˆì•¡', 'ì£¼ìš”_ì—°ë ¹ëŒ€', 'predicted_turnover']
        available_cols = [col for col in display_cols if col in top_areas.columns]
        st.dataframe(top_areas[available_cols].style.highlight_max(axis=0))
    
    with tab4:
        st.header("ğŸ’¡ ìµœì¢… ë¶„ì„ ê²°ê³¼ ë° ì œì•ˆ")
        
        # í•µì‹¬ ì¸ì‚¬ì´íŠ¸
        st.markdown("### ğŸ¯ í•µì‹¬ ë°œê²¬")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.success(f"""
            **1. ìµœì  íƒ€ê²Ÿ ìƒê¶Œ ìœ í˜•**
            - {[k for k, v in cluster_names.items() if 'íŠ¸ë Œë””' in v][0]}ë²ˆ í´ëŸ¬ìŠ¤í„° (íŠ¸ë Œë””í˜•)
            - íŠ¹ì§•: ë†’ì€ ì—…ì¢… ë‹¤ì–‘ì„±, 20-30ëŒ€ ì¤‘ì‹¬, í™œë°œí•œ ì°½ì—…
            """)
            
            st.info(f"""
            **2. ìœ ë§ ìƒê¶Œ íŠ¹ì„±**
            - í‰ê·  ê²°ì œê¸ˆì•¡ì´ ë‚®ì€ ì§€ì—­ (ì†Œì•¡ ë‹¤ë¹ˆë„)
            - 20ëŒ€ ë§¤ì¶œ ë¹„ì¤‘ì´ ë†’ì€ ì§€ì—­
            - ì—…ì¢… ë‹¤ì–‘ì„±ì´ ë†’ì€ ì§€ì—­
            """)
        
        with col2:
            # TOP 3 ì§€ì—­ ì •ë³´
            top1_name = top_areas.iloc[0]['dong_name'] if len(top_areas) > 0 else 'N/A'
            top1_rate = top_areas.iloc[0]['predicted_turnover'] if len(top_areas) > 0 else 0
            top2_name = top_areas.iloc[1]['dong_name'] if len(top_areas) > 1 else 'N/A'
            top2_rate = top_areas.iloc[1]['predicted_turnover'] if len(top_areas) > 1 else 0
            top3_name = top_areas.iloc[2]['dong_name'] if len(top_areas) > 2 else 'N/A'
            top3_rate = top_areas.iloc[2]['predicted_turnover'] if len(top_areas) > 2 else 0
            
            st.warning(f"""
            **3. ìµœìš°ì„  ë³´ê¸‰ ì§€ì—­ TOP 3**
            1. **{top1_name}** 
               - ì˜ˆì¸¡ íšŒì „ìœ¨: {top1_rate:.1%}
            2. **{top2_name}**
               - ì˜ˆì¸¡ íšŒì „ìœ¨: {top2_rate:.1%}
            3. **{top3_name}**
               - ì˜ˆì¸¡ íšŒì „ìœ¨: {top3_rate:.1%}
            """)
        
        # ì‹¤í–‰ ì „ëµ
        st.markdown("### ğŸ“Š ì‹¤í–‰ ì „ëµ")
        
        strategy_df = pd.DataFrame({
            'ìš°ì„ ìˆœìœ„': ['1ìˆœìœ„', '2ìˆœìœ„', '3ìˆœìœ„'],
            'ëŒ€ìƒ ì§€ì—­': [
                'ì„±ìˆ˜ë™, ì—°ë‚¨ë™ ë“± íŠ¸ë Œë”” ìƒê¶Œ',
                'ì—­ì‚¼ë™, ì„ ë¦‰ ë“± ì˜¤í”¼ìŠ¤ ë°€ì§‘ ìƒê¶Œ',
                'ëŒ€ì¹˜ë™, ëª©ë™ ë“± ì£¼ê±° ë°€ì§‘ ìƒê¶Œ'
            ],
            'ì˜ì—… ì „ëµ': [
                'ì‹ ê·œ ì°½ì—… ì‚¬ì „ ì»¨íƒ, ì˜¤í”ˆ í”„ë¡œëª¨ì…˜',
                'ì ì‹¬ì‹œê°„ ì§‘ì¤‘ ì˜ì—…, B2B íŒ¨í‚¤ì§€',
                'í”„ëœì°¨ì´ì¦ˆ ë³¸ì‚¬ ê³µëµ, ì¥ê¸° ê³„ì•½'
            ],
            'ì˜ˆìƒ ë³´ê¸‰ë¥ ': ['25%', '15%', '10%']
        })
        
        st.table(strategy_df)
        
        # ROI ì˜ˆì¸¡
        st.markdown("### ğŸ’° ì˜ˆìƒ ROI")
        
        metrics = st.columns(4)
        with metrics[0]:
            st.metric("ëª©í‘œ ì‹ ê·œ ê°€ë§¹ì ", "1,000ê°œ", "+40%")
        with metrics[1]:
            st.metric("ì˜ˆìƒ ì›” ê±°ë˜ì•¡", "50ì–µì›", "+25%")
        with metrics[2]:
            st.metric("ì˜ˆìƒ ìˆ˜ìˆ˜ë£Œ ìˆ˜ìµ", "1.5ì–µì›/ì›”", "+30%")
        with metrics[3]:
            st.metric("íˆ¬ì íšŒìˆ˜ ê¸°ê°„", "8ê°œì›”", "-4ê°œì›”")
        
        # ë‹¤ìŒ ë‹¨ê³„
        st.markdown("### ğŸš€ Next Steps")
        st.markdown("""
        1. **ì¦‰ì‹œ ì‹¤í–‰ (1ì£¼ì¼ ë‚´)**
           - ì„±ìˆ˜ë™, ì—°ë‚¨ë™ í˜„ì¥ ì¡°ì‚¬
           - ì‹ ê·œ ì°½ì—… ì˜ˆì •ì ë¦¬ìŠ¤íŠ¸ í™•ë³´
           
        2. **ë‹¨ê¸° ì‹¤í–‰ (1ê°œì›” ë‚´)**
           - íŠ¸ë Œë”” ìƒê¶Œ ì „ë‹´íŒ€ êµ¬ì„±
           - íƒ€ê²Ÿ ì§€ì—­ í”„ë¡œëª¨ì…˜ ê¸°íš
           
        3. **ì¤‘ê¸° ì‹¤í–‰ (3ê°œì›” ë‚´)**
           - ë°ì´í„° ê¸°ë°˜ ì˜ì—… ì‹œìŠ¤í…œ êµ¬ì¶•
           - ì„±ê³¼ ì¸¡ì • ë° ì „ëµ ì¡°ì •
        """)

if __name__ == "__main__":
    main()



